{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "num_epochs = 30\n",
    "batch_size = 1\n",
    "num_trainimgs = 234\n",
    "num_valimgs = 26\n",
    "#decay_step = int(num_trainimgs / batch_size * 10)\n",
    "decay_step = 10000\n",
    "decay_rate = 0.75\n",
    "seed = 777\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "data_dir = cur_dir\n",
    "TRAIN_FILE = 'train_images.tfrecords'\n",
    "VALIDATION_FILE = 'val_images.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_images.tfrecoreds와 test_images.tfrecords file upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uploading files from your local file system to Colab\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "        name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensor_shape(tensor, string):\n",
    "    print(string, tensor.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tfrecord parsing\n",
    "def read_and_decode(tfrecord_serialized):\n",
    "    features={'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "             'label_raw': tf.FixedLenFeature([], tf.string)}\n",
    "    parsed_features = tf.parse_single_example(tfrecord_serialized, features)\n",
    "    \n",
    "    image = tf.decode_raw(parsed_features['img_raw'], tf.int64)\n",
    "    image.set_shape([65536])\n",
    "    image_re = tf.reshape(image, [256, 256])\n",
    "    image_re = tf.cast(image_re, tf.float32) * (1. / 1024)\n",
    "    label = tf.decode_raw(parsed_features['label_raw'], tf.uint8)\n",
    "    label.set_shape([65536])\n",
    "    label_re = tf.reshape(label, [256, 256])\n",
    "    \n",
    "    return image_re, label_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 구성\n",
    "def make_dataset(batch_size, tfrecord_path):    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    dataset = dataset.map(read_and_decode, num_parallel_calls=8)\n",
    "    dataset = dataset.shuffle(buffer_size=10000).prefetch(buffer_size=batch_size).batch(batch_size).repeat()\n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network 구성 - simplified U-Net\n",
    "def network(images):\n",
    "    print_tensor_shape(images, 'input images shape')\n",
    "    image_re = tf.reshape(images, [-1, 256, 256, 1])\n",
    "    print_tensor_shape(images, 'input images shape after reshaping')\n",
    "    \n",
    "    ## Contraction\n",
    "    conv1 = tf.layers.conv2d(image_re,\n",
    "                             filters=64,\n",
    "                             kernel_size=[3,3],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv1')\n",
    "    print_tensor_shape(conv1, 'conv1 shape')\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(conv1,\n",
    "                             filters=128,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv2')\n",
    "    print_tensor_shape(conv2, 'conv2 shape')\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(conv2,\n",
    "                             filters=256,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv3')\n",
    "    print_tensor_shape(conv3, 'conv3 shape')\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(conv3,\n",
    "                             filters=512,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv4')\n",
    "    print_tensor_shape(conv4, 'conv4 shape')\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(conv4,\n",
    "                             filters=1024,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv5')\n",
    "    print_tensor_shape(conv5, 'conv5 shape')\n",
    "    \n",
    "    ## Expansion\n",
    "    upconv6 = tf.layers.conv2d_transpose(conv5,\n",
    "                             filters=512,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='upconv6')\n",
    "    print_tensor_shape(upconv6, 'upconv6 shape')\n",
    "    \n",
    "    concat6 = tf.concat([conv4, upconv6], axis=-1)\n",
    "    \n",
    "    conv6 = tf.layers.conv2d(concat6,\n",
    "                             filters=512,\n",
    "                             kernel_size=[3,3],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv6')\n",
    "    print_tensor_shape(conv6, 'conv6 shape')\n",
    "    \n",
    "    upconv7 = tf.layers.conv2d_transpose(conv6,\n",
    "                             filters=256,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='upconv7')\n",
    "    print_tensor_shape(upconv7, 'upconv7 shape')\n",
    "    \n",
    "    concat7 = tf.concat([conv3, upconv7], axis=-1)\n",
    "    \n",
    "    conv7 = tf.layers.conv2d(concat7,\n",
    "                             filters=256,\n",
    "                             kernel_size=[3,3],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv7')\n",
    "    print_tensor_shape(conv7, 'conv7 shape')\n",
    "    \n",
    "    upconv8 = tf.layers.conv2d_transpose(conv7,\n",
    "                             filters=128,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='upconv8')\n",
    "    print_tensor_shape(upconv8, 'upconv8 shape')\n",
    "    \n",
    "    concat8 = tf.concat([conv2, upconv8], axis=-1)\n",
    "    \n",
    "    conv8 = tf.layers.conv2d(concat8,\n",
    "                             filters=128,\n",
    "                             kernel_size=[3,3],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv8')\n",
    "    print_tensor_shape(conv8, 'conv8 shape')\n",
    "    \n",
    "    upconv9 = tf.layers.conv2d_transpose(conv8,\n",
    "                             filters=64,\n",
    "                             kernel_size=[5,5],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[2,2],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='upconv9')\n",
    "    print_tensor_shape(upconv9, 'upconv9 shape')\n",
    "    \n",
    "    concat9 = tf.concat([conv1, upconv9], axis=-1)\n",
    "    \n",
    "    conv9 = tf.layers.conv2d(concat9,\n",
    "                             filters=64,\n",
    "                             kernel_size=[3,3],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv9')\n",
    "    print_tensor_shape(conv9, 'conv9 shape')\n",
    "    \n",
    "    conv10 = tf.layers.conv2d(conv9,\n",
    "                             filters=64,\n",
    "                             kernel_size=[3,3],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],\n",
    "                             activation=tf.nn.relu,\n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv10')\n",
    "    print_tensor_shape(conv10, 'conv10 shape')\n",
    "    \n",
    "    conv11 = tf.layers.conv2d(conv10,\n",
    "                             filters=2,\n",
    "                             kernel_size=[1,1],\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed),\n",
    "                             strides=[1,1],                             \n",
    "                             use_bias=False,\n",
    "                             padding='same',\n",
    "                             name='conv11')\n",
    "    print_tensor_shape(conv11, 'conv11 shape')\n",
    "    \n",
    "    return conv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function 정의 - cross entropy\n",
    "def loss(logits, labels):\n",
    "    labels = tf.to_int64(labels)\n",
    "    print_tensor_shape(logits, 'logits shape before')\n",
    "    print_tensor_shape(labels, 'labels shape before')\n",
    "    \n",
    "    logits_re = tf.reshape(logits, [-1, 2])\n",
    "    labels_re = tf.reshape(labels, [-1])\n",
    "    print_tensor_shape(logits_re, 'logits shape after')\n",
    "    print_tensor_shape(labels_re, 'labels shale after')\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy')\n",
    "    print_tensor_shape(cross_entropy, 'cross_entropy shape')\n",
    "    \n",
    "    loss = tf.reduce_mean(cross_entropy, name='simple_cross_entropy_mean')\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient descent with learning rate decay\n",
    "def training(loss, learning_rate, decay_steps, decay_rate):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)    \n",
    "    lr = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)    \n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate accuracy\n",
    "def evaluation(logits, labels):\n",
    "    with tf.name_scope('eval'):\n",
    "        labels = tf.to_int64(labels)\n",
    "        print_tensor_shape( logits, 'logits eval shape before')\n",
    "        print_tensor_shape( labels, 'labels eval shape before')\n",
    "\n",
    "        logits_re = tf.reshape( logits, [-1, 2] )\n",
    "        labels_re = tf.reshape( labels, [-1] )\n",
    "        print_tensor_shape( logits, 'logits eval shape after')\n",
    "        print_tensor_shape( labels, 'labels eval shape after')\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits_re, labels_re, 1)\n",
    "        print_tensor_shape( correct, 'correct shape')\n",
    "\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfr_path = os.path.join(data_dir, TRAIN_FILE)\n",
    "val_tfr_path = os.path.join(data_dir, VALIDATION_FILE)\n",
    "\n",
    "train_dataset = make_dataset(batch_size, train_tfr_path)\n",
    "val_dataset = make_dataset(batch_size, val_tfr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "images, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init = iterator.make_initializer(train_dataset)\n",
    "val_init = iterator.make_initializer(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = training(loss, learning_rate, decay_step, decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_op = evaluation(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(train_init)\n",
    "num_fig = 5\n",
    "plt.figure(figsize=(10,25))\n",
    "for i in range(num_fig):\n",
    "    tmp_image, tmp_label = sess.run([images, labels])\n",
    "    plt.subplot(num_fig, 2, 2*i+1)\n",
    "    plt.title('MRI image')\n",
    "    plt.imshow(tmp_image[0], cmap='gray', vmin=0, vmax=1)\n",
    "    plt.subplot(num_fig, 2, 2*i+2)\n",
    "    plt.title('LV segmentation')\n",
    "    plt.imshow(tmp_label[0], cmap='gray', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_prec_train = 0.\n",
    "    avg_prec_val = 0.\n",
    "    n_iter_train = int(num_trainimgs / batch_size)\n",
    "    n_iter_val = int(num_valimgs / batch_size)\n",
    "    \n",
    "    sess.run(train_init)    \n",
    "    for i in range(n_iter_train):\n",
    "        #sess.run([images, labels])\n",
    "        _, loss_val = sess.run([train_op, loss])\n",
    "        prec = sess.run(eval_op)\n",
    "        avg_loss += loss_val / n_iter_train\n",
    "        avg_prec_train += prec / (n_iter_train * 256.0 * 256)\n",
    "    \n",
    "    sess.run(val_init)\n",
    "    for i in range(n_iter_val):\n",
    "        val_images, val_labels = sess.run([images, labels])\n",
    "        val_logits, prec = sess.run([logits, eval_op])\n",
    "        avg_prec_val += prec / (n_iter_val * 256.0 * 256)\n",
    "        if ((epoch+1) % 10 == 0):            \n",
    "            val_images = np.reshape(val_images, (256, 256))\n",
    "            val_labels = np.reshape(val_labels, (256, 256))\n",
    "            val_logits = np.reshape(val_logits[:,:,:,1], (256, 256))\n",
    "            \n",
    "            plt.figure(figsize=(10,50))\n",
    "            plt.subplot(131)\n",
    "            plt.title('MRI image')\n",
    "            plt.imshow(val_images, cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(132)\n",
    "            plt.title('LV label')\n",
    "            plt.imshow(val_labels, cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(133)\n",
    "            plt.title('LV inference')\n",
    "            plt.imshow(val_logits, cmap='gray', vmin=0, vmax=1)\n",
    "            plt.show()    \n",
    "    \n",
    "    print('OUTPUT: epoch {}: loss = {:.5f}, train_precision = {:.3f}, val_precision = {:.3f}'.format(        epoch+1, avg_loss, avg_prec_train, avg_prec_val ))\n",
    "\n",
    "print('Done Training for {} epochs'.format(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
